# Static configuration
cuda_visible_devices: "0, 1" # we only have 1 GPU

model_name: "mistralai/Mistral-7B-Instruct-v0.3"
model_weight_dir: "/home/ubuntu/MLP-RLHF/Self-Rewarding-Language-Models/results/results_2025-03-24_16-14-55/dpo/iteration_1"
tokenizer_name: "mistralai/Mistral-7B-Instruct-v0.3"

data_directory: "data"
# ift_dataset: "srlm_ift.jsonl"
ift_dataset: "HealthCareMagic-3k-en.jsonl"
model_directory: "results"

wandb_enable: True
wandb_project: "Self Rewarding Language Models"

peft_config:
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
  lora_dropout: 0.5
  lora_alpha: 16
  lora_r: 16

iterations: 2 # 5

sft_training:
  learning_rate: 5e-5
  batch_size: 4
  max_seq_length: 1024 # 1024

dpo_training:
  learning_rate: 5e-5
  batch_size: 4
  max_seq_length: 1024 # 1024
  max_prompt_length: 1024 # 1024

generate_prompts:
  new_prompts: 100 # 500

response_prompts:
  new_prompts: 4